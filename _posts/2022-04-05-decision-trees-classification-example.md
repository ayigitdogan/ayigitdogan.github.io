---
title: Decision Trees - Classification Example
date: 2022-04-05 14:10:00 +0800
categories: [Engineering, Machine Learning]
tags: [classification, r, tree based algorithms]
render_with_liquid: false
---

## Introduction

In this exercise, predictions on the financial conditions of the
companies given in the data set *Financialdistress-cat.csv* will be
held. The output attribute to be predicted is the Financial.Distress
attribute, which is zero if the company is in a healthy condition, one
otherwise.

One can check the [GitHub repository](https://github.com/ayigitdogan/Decision-Trees-Classification-Example)
for further details.

The first step is importing the required libraries and the data set.

``` r
library(rpart)
library(rpart.plot)
library(caret)
library(tree)
library(caTools)
library(dplyr)
library(Metrics)

fd <- read.csv("FinancialDistress-cat.csv")
fd$Financial.Distress <- as.factor(fd$Financial.Distress)

# Splitting the data set into training and test

set.seed(425)

split1  <- sample.split(fd$Financial.Distress, SplitRatio = 0.75)

fdtrain <- subset(fd, split1==TRUE)
fdtest  <- subset(fd, split1==FALSE)

prcntfd   <- nrow(dplyr::filter(fd,      Financial.Distress %in% 1)) / nrow(fd)      
prcnttr   <- nrow(dplyr::filter(fdtrain, Financial.Distress %in% 1)) / nrow(fdtrain)
prcnttest <- nrow(dplyr::filter(fdtest,  Financial.Distress %in% 1)) / nrow(fdtest)

percentages        <- 100*c(prcntfd  , prcnttr   , prcnttest )
names(percentages) <-     c("overall", "training", "test"    )

knitr::kable(round(percentages, 3), "simple", col.names = "Distress %")
```

<div align="center">

 <table>
  <tr>
    <th> </th>
    <th>Distress %</th>
  </tr>
  <tr>
    <td>overall</td>
    <td>6.863</td>
  </tr>
  <tr>
    <td>training</td>
    <td>6.863</td>
  </tr>
    <tr>
    <td>test</td>
    <td>6.863</td>
  </tr>
</table>

</div>
  
<p style="text-align: center;"><em>Table 1. Percentage of companies in distress in the original and split data sets</em></p>

The percentages look pretty much the same, which implies that the data
set is suitable for a homogeneous split with high precision.

## Generating and Pruning the Tree (Based on the Cross-Validation Error)

The next step is to determine the best size of the best tree in terms of
cross validation error.

``` r
# Generating the tree (Parameters are chosen intuitively and also by trial and error.)

treeA <- rpart(Financial.Distress~.,
               data      = fdtrain,
               minsplit  = 30,
               minbucket = 8)

prp(treeA,
    type    = 5,
    extra   = 1,
    tweak   = 1)
```

![Figure 1](/assets/img/content/220405/rawtree-1.png)  
<p style="text-align: center;"><em>Figure 1. Decision tree before pruning</em></p>

Checking the CP table:

``` r
cpTable <- printcp(treeA)
knitr::kable(cpTable, "simple", row.names = FALSE)
```

<div align="center">

<table>
<thead>
  <tr>
    <th>CP</th>
    <th>nsplit</th>
    <th>rel error</th>
    <th>xerror</th>
    <th>xstd</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td>0.1216931</td>
    <td>0</td>
    <td>1.0000000</td>
    <td>1.0000000</td>
    <td>0.0701990</td>
  </tr>
  <tr>
    <td>0.1005291</td>
    <td>1</td>
    <td>0.8783069</td>
    <td>1.0529101</td>
    <td>0.0718916</td>
  </tr>
  <tr>
    <td>0.0476190</td>
    <td>2</td>
    <td>0.7777778</td>
    <td>0.8571429</td>
    <td>0.0653328</td>
  </tr>
  <tr>
    <td>0.0423280</td>
    <td>3</td>
    <td>0.7301587</td>
    <td>0.8888889</td>
    <td>0.0664546</td>
  </tr>
  <tr>
    <td>0.0185185</td>
    <td>4</td>
    <td>0.6878307</td>
    <td>0.8306878</td>
    <td>0.0643787</td>
  </tr>
  <tr>
    <td>0.0158730</td>
    <td>6</td>
    <td>0.6507937</td>
    <td>0.8783069</td>
    <td>0.0660834</td>
  </tr>
  <tr>
    <td>0.0132275</td>
    <td>7</td>
    <td>0.6349206</td>
    <td>0.9100529</td>
    <td>0.0671891</td>
  </tr>
  <tr>
    <td>0.0105820</td>
    <td>9</td>
    <td>0.6084656</td>
    <td>0.9259259</td>
    <td>0.0677331</td>
  </tr>
  <tr>
    <td>0.0100000</td>
    <td>10</td>
    <td>0.5978836</td>
    <td>0.9523810</td>
    <td>0.0686273</td>
  </tr>
</tbody>
</table>

</div>

<p style="text-align: center;"><em>Table 2. CP Table</em></p>

Pruning the tree:

``` r
# Reporting the number of terminal nodes in the tree with the lowest cv-error, 
# which is equal to [the number of splits performed to create the tree] + 1

optIndex <- which.min(unname(treeA$cptable[, "xerror"]))

cpTable[optIndex, 2] + 1
```

    ## [1] 5

``` r
# Pruning the tree to the optimized cp value

optTree <- prune.rpart(tree = treeA,
                       cp   = cpTable[optIndex, 1])

prp(optTree)
```
![Figure 2](/assets/img/content/220405/prune-1.png)  
<p style="text-align: center;"><em>Figure 2. Decision tree after pruning</em></p>

As the cp table generated by the R script suggests, the tree with 2
splits and 3 terminal nodes yields the lowest cross-validation error.

## Predictions (Minimizing CV Error)

Predictions can be made in the test set along with reporting the error
rate, sensitivity, specificity, and precision using the
*confusionMatrix* function of the caret package.

``` r
# Making predictions in the test set and tabulating the results

predA    <- predict(optTree,
                    newdata = fdtest,
                    type = "class")

tblA     <- table(fdtest$Financial.Distress,
                  predA)

knitr::kable(tblA, "simple", col.names = c("pred_0", "pred_1"))
```

<div align="center">

 <table>
  <tr>
    <th> </th>
    <th>pred_0</th>
    <th>pred_1</th>
  </tr>
  <tr>
    <td>0</td>
    <td>842</td>
    <td>13</td>
  </tr>
  <tr>
    <td>1</td>
    <td>37</td>
    <td>26</td>
  </tr>
</table> 

</div>

<p style="text-align: center;"><em>Table 3. Prediction results</em></p>

``` r
# Generating the confusion matrix

cmA <- confusionMatrix(predA, fdtest$Financial.Distress, positive = "1")
    
# Reporting the metrics

results <- matrix(c(cmA[["overall"]][["Accuracy"]]   ,
                    cmA[["byClass"]][["Sensitivity"]],
                    cmA[["byClass"]][["Specificity"]],
                    cmA[["byClass"]][["Precision"]]  ), ncol = 1)

rownames(results) <- c('Accuracy', 'Sensitivity', "Specificity", "Precision")

knitr::kable(round(results, 3)*100, "simple", col.names = "%")
```

<div align="center">
 
 <table>
  <tr>
    <th> </th>
    <th>%</th>
  </tr>
  <tr>
    <td>Accuracy</td>
    <td>94.6</td>
  </tr>
  <tr>
    <td>Sensitivity</td>
    <td>41.3</td>
  </tr>
  <tr>
    <td>Specificity</td>
    <td>98.5</td>
  </tr>
  <tr>
    <td>Precision</td>
    <td>66.7</td>
  </tr>
</table> 

</div>

<p style="text-align: center;"><em>Table 4. Prediction report</em></p>

## Pruning the Tree (Based on the Cost Complexity)

The raw tree can be alternatively pruned by minimizing the cost
complexity, which is measured by the deviance in the tree package.

To obtain the tree with the smallest deviance, the *minsize* and
*mindev* parameters should be set to the minimum values they can take, 2
and 0, in the *tree* function.

``` r
# Creating a tree with terminal nodes that all have zero deviance

treeB <- tree(Financial.Distress~.,
              data      = fdtrain,
              minsize   = 2,
              mindev    = 0.0)

# Reporting the number of terminal nodes

summary(treeB)[["size"]]
```

    ## [1] 89

## Predictions (Minimizing Cost Complexity)

``` r
# Making predictions in the test set and tabulating the results

predB   <- predict(treeB,
                   newdata = fdtest,
                   type = "class")

tblB    <- table(fdtest$Financial.Distress, predB)

knitr::kable(tblB, "simple", col.names = c("pred_0", "pred_1"))
```

<div align="center">

 <table>
  <tr>
    <th> </th>
    <th>pred_0</th>
    <th>pred_1</th>
  </tr>
  <tr>
    <td>0</td>
    <td>823</td>
    <td>24</td>
  </tr>
  <tr>
    <td>1</td>
    <td>39</td>
    <td>24</td>
  </tr>
</table> 

</div>

<p style="text-align: center;"><em>Table 5. Prediction results (alternative pruning)</em></p>

``` r
# Generating the confusion matrix

cmB <- confusionMatrix(predB, fdtest$Financial.Distress, positive = "1")

# Reporting the metrics and comparison with part C

results <- cbind(results, c(cmB[["overall"]][["Accuracy"]],
                            cmB[["byClass"]][["Sensitivity"]],
                            cmB[["byClass"]][["Specificity"]],
                            cmB[["byClass"]][["Precision"]]   ))

knitr::kable(round(results, 3), "simple", col.names = c("Cross-Validation", "Cost Complexity"))
```

<div align="center">

 <table>
  <tr>
    <th> </th>
    <th>Cross-Validation</th>
    <th>Cost Complexity</th>
  </tr>
  <tr>
    <td>Accuracy</td>
    <td>0.946</td>
    <td>0.923</td>
  </tr>
  <tr>
    <td>Sensitivity</td>
    <td>0.413</td>
    <td>0.381</td>
  </tr>
  <tr>
    <td>Specificity</td>
    <td>0.985</td>
    <td>0.963</td>
  </tr>
  <tr>
    <td>Precision</td>
    <td>0.667</td>
    <td>0.429</td>
  </tr>
</table> 

</div>

<p style="text-align: center;"><em>Table 6. Performance comparison of the two models</em></p>

As can be expected, the tree pruned by minimizing the cross-validation
error has better accuracy, specificity, and precision compared to the
one pruned according to cost complexity, which has lost some of its
general validity due to its perfect fit. In other words, too many
terminal nodes might lead the model to overfitting!

*Written by Ahmet Yiğit Doğan*  
*IE 425 - Data Mining*  
*Boğaziçi University - Industrial Engineering Department*  
[GitHub Repository](https://github.com/ayigitdogan/Decision-Trees-Classification-Example)
